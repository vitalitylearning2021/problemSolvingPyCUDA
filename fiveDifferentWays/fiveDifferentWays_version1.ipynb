{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fiveDifferentWays_version1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD4jH3ZotYpS",
        "colab_type": "text"
      },
      "source": [
        "# PyCUDA installation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDdNH1CBvz0",
        "colab_type": "code",
        "outputId": "be3a07b9-cd36-4766-e937-94b60ce30868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/3f/5658c38579b41866ba21ee1b5020b8225cec86fe717e4b1c5c972de0a33c/pycuda-2019.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 16.3MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/c7/88a4f8b6f0f78d0115ec3320861a0cc1f6daa3b67e97c3c2842c33f9c089/pytools-2020.1.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/78/f6ade1e18aebda570eed33b7c534378d9659351cadce2fcbc7b31be5f615/Mako-1.1.2-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2019.1.2-cp36-cp36m-linux_x86_64.whl size=4537514 sha256=2a3b8a102148d8de901cabac804674b5ed1be89bcded0d93ffe7ffe2ad3ef2b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/60/f0/b1c430c73d281ac3e46070480db50f7907364eb6f6d3188396\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.1-py2.py3-none-any.whl size=59602 sha256=ab844ce2ae06eee1c59f06d97f693778d02ace89d399b315c10ed4097a513068\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/da/1b/946775a88291378182ed92c9800d6d0ebc2a554cb89829cc24\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.1.2 pycuda-2019.1.2 pytools-2020.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpqdkWPZDXo7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvlvVH34tnWL",
        "colab_type": "text"
      },
      "source": [
        "# Version #1: using ```SourceModule```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4RYnD8i89Op",
        "colab_type": "text"
      },
      "source": [
        "PyCUDA initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xYQK81g81vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- PyCUDA initialization\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv9xrA8_9VUa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "iDivUp function: if ```b``` divides ```a```, then ```a/b``` is returned, otherwise the function returns the integer division between ```a``` and ```b``` ```+1```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB-1igHCBM9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# iDivUp FUNCTION #\n",
        "###################\n",
        "def iDivUp(a, b):\n",
        "    # Round a / b to nearest higher integer value\n",
        "    a = np.int32(a)\n",
        "    b = np.int32(b)\n",
        "    return (a / b + 1) if (a % b != 0) else (a / b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP7b2khB-Jha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########\n",
        "# MAIN #\n",
        "########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjO-Y74f-nJC",
        "colab_type": "text"
      },
      "source": [
        "Defining two CUDA events that will be used to measure execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcTqVeoT-MdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = cuda.Event()\n",
        "end   = cuda.Event()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xgfClb_EQa",
        "colab_type": "text"
      },
      "source": [
        "Number of array elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bptd0phi-6vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1fEQDh_Ytq",
        "colab_type": "text"
      },
      "source": [
        "Number of threads per block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-RWRjai_DGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BLOCKSIZE = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8BIZ_i_mRC",
        "colab_type": "text"
      },
      "source": [
        "Create two host vectors ```h_a``` and ```h_b``` of ```N``` random entries. ```np.random.randn``` returns ```float64```'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JX1Sj20_LlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_a = np.random.randn(1, N)\n",
        "h_b = np.random.randn(1, N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOaEH68DFq2",
        "colab_type": "text"
      },
      "source": [
        "Cast ```h_a``` and ```h_b``` to single precision (```float32```). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mZd8jV_k_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_a = h_a.astype(np.float32)\n",
        "h_b = h_b.astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5st--rgFDqB8",
        "colab_type": "text"
      },
      "source": [
        "Allocate ```h_a.nbytes```, ```h_b.nbytes``` and ```h_c.nbytes``` of GPU device memory space pointed to by ```d_a```, ```d_b``` and ```d_c```, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICyUxA9CA3Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_a = cuda.mem_alloc(h_a.nbytes)\n",
        "d_b = cuda.mem_alloc(h_b.nbytes)\n",
        "d_c = cuda.mem_alloc(h_a.nbytes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_20z0qpErTs",
        "colab_type": "text"
      },
      "source": [
        "Copy the ```h_a``` and ```h_b``` arrays from host to the device arrays ```d_a``` and ```d_b```, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlJJ2aYHD7Ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda.memcpy_htod(d_a, h_a)\n",
        "cuda.memcpy_htod(d_b, h_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b_lsPJTFKE1",
        "colab_type": "text"
      },
      "source": [
        "Define the CUDA kernel function ```deviceAdd``` as a string. ```deviceAdd``` performs the elementwise summation of ```d_a``` and ```d_b``` and puts the result in ```d_c```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0JSS1BvEo2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  #include <stdio.h>\n",
        "  __global__ void deviceAdd(float * __restrict__ d_c, const float * __restrict__ d_a, const float * __restrict__ d_b, const int N)\n",
        "  {\n",
        "    const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "    d_c[tid] = d_a[tid] + d_b[tid];\n",
        "  } \n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8He90xF2Ul",
        "colab_type": "text"
      },
      "source": [
        "Define a reference to the ```__global__``` function ```deviceAdd```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsPvYXgrE48k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deviceAdd = mod.get_function(\"deviceAdd\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnW0kx9GXbN",
        "colab_type": "text"
      },
      "source": [
        "Define the block size ```blockDim``` and the grid size ```gridDim```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gJ4KvepF1cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blockDim  = (BLOCKSIZE, 1, 1)\n",
        "gridDim   = (int(iDivUp(N, BLOCKSIZE)), 1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKvUKMC_IIL_",
        "colab_type": "text"
      },
      "source": [
        "Invoke the ```deviceAdd``` function. \n",
        "Note that, up to here, ```N``` is an *object* of ```class int``` and not an integer number. Therefore, before using it, we must cast it to ```np.int32``` which is essentially the standard, single precision, floating point type.\n",
        "Before launching ```deviceAdd```, the ```start``` and ```end``` events are recorded, so that the execution time can be measured.\n",
        "Note that, before the processing time can be measured, all the activities in the current context must be ceased. This is the reason why ```end.synchronize()``` is invoked. Remember that the host and device executions are asynchronous. Furthermore, with the event record, the device will record a time stamp for the event when it reaches that event in the stream. Without synchronization, it happens that the ```end``` event is recorded after the ```deviceAdd``` function execution is actually terminated, as we expect, but the ```print``` function is executed before ```deviceAdd``` has actually finished its execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddx_KUikGTPH",
        "colab_type": "code",
        "outputId": "bc2cf6b8-7586-47a5-a6ef-7b0586e7d1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# --- Warmup execution\n",
        "deviceAdd(d_c, d_a, d_b, np.int32(N), block = blockDim, grid = gridDim)\n",
        "\n",
        "start.record()\n",
        "deviceAdd(d_c, d_a, d_b, np.int32(N), block = blockDim, grid = gridDim)\n",
        "end.record() \n",
        "end.synchronize()\n",
        "secs = start.time_till(end) * 1e-3\n",
        "print(\"Processing time = %fs\" % (secs))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing time = 0.000517s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjCl9RrSHvK",
        "colab_type": "text"
      },
      "source": [
        "Allocate host space and copy results from device to host."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wySMagLqGumX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_c = np.empty_like(h_a)\n",
        "cuda.memcpy_dtoh(h_c, d_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs-9eC0YSpiS",
        "colab_type": "text"
      },
      "source": [
        "Check if the device processing results are as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVWn4GRSF0j",
        "colab_type": "code",
        "outputId": "fad7115d-1633-4dd3-92bb-f38c9cbc8f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if np.array_equal(h_c, h_a + h_b):\n",
        "  print(\"Test passed!\")\n",
        "else :\n",
        "  print(\"Error!\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQUSGVoSv6i",
        "colab_type": "text"
      },
      "source": [
        "Finally, flush context printf buffer. Without flushing, no printout may be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Vvif3iSP2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda.Context.synchronize()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
